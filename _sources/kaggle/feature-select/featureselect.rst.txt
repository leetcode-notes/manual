feature select 特徴選択
===========================

センサー（特徴量）が異常検知（ターゲット）に対してどれくらい役立つかを調べて、

機械学習モデルへ学習させるセンサーを選別する作業

同じく特徴量を減らす方法に特徴量エンジニアリングという手法もあります


特徴選択の最大の目的
--------------------------

+ 学習データを縮小することによって学習にかかる時間を短縮できる

+ モデルの構造を単純化し理解しやすくできる

+ 過学習を防ぐことができる

特徴選択の一般的な3種類の手法
--------------------------------

フィルタ法（Filter Method）
--------------------------------

統計のテクニックを用いて各特徴の「予測に使える度合」を点数化

点数を決めるのにもいくつか方法があり、特徴と予測対象の関係性を見て決める方法

もあれば、特徴だけを見て統計的に決める方法もあります。

欠点としては、特徴を1つずつしか見られないので、複数の特徴量の併用効果は考慮されないことが挙げられます。

（例：センサーAとセンサーBを併用して異常検知を行うと精度が向上するがフィルタ法では考慮されない）

各特徴量の点数を決める方法ですが、特徴量とターゲットのデータ型により手法が異なります。

主な手法としては「カイ二乗検定（Chi-Square）」や「ANOVA（Analytics of Variance）」などがあります。

機械学習クラウドサービスのマイクロソフトAzure Machine Learning Studioでは

フィルタ法として7つの手法が用意されています。

このように知識と経験が必要な特徴選択のプロセスを単純化して実装できるのは、クラウドサービスの良いメリット

ラッパー法（Wrapper Method）
------------------------------

ラッパー法では複数の特徴を同時に使って予測精度の検証を行い、

精度が最も高くなるような特徴量の組み合わせを探索していきます。

様々な組み合わせでそれぞれ学習を行わせ、その学習結果をもとに組み合わせに優劣をつけていきます。

様々な特徴量を使って予測を繰り返して、精度が高くなる特徴量へ絞っていく手法

計算方法は複数の手法
----------------------

下記は基本的な2つの手法

前進法（Forward Search）
--------------------------

前進法では、まず全ての特徴量を学習データから取り除いた状態からスタートします。

そこから、精度の向上が一番大きくなるような、最も有用な特徴を1つずつ足していきます。

これを、精度の変化がなくなるまで反復的に繰り返します。

後退法（Backward Elimination）
-----------------------------------

後退法では、まず全ての特徴を学習データに含めた状態からスタートします。

そこから、精度の向上が一番大きくなるような、最も不要な特徴を1つずつ取り除いていきます。

これを、精度の変化がなくなるまで反復的に繰り返します。

組み込み法（Embedded Method）
--------------------------------

フィルタ法とラッパー法の2つの強みを掛け合わせたような手法で、

機械学習モデルが学習の一環として特徴の選択を行います。

上2つの方法と違い、そもそもの学習アルゴリズムに特徴量選択が組み込まれているので、

学習と特徴量選択を同時に行うことができます。

使われるアルゴリズムですが「ラッソ回帰（LASSO Linear Regression）」や「決定木」などがあります。


