L-BFGS
=============================

.. sidebar:: 目次

   .. contents::
       :depth: 3
       :local:


.. figure:: 1.png

凸最適化問題
---------------

L-BFGSは凸最適化問題を効率よく解くことができ、scikit-learnやsparkの
線形モデル(logistic回帰など)のパラメータ推定など、広く用いられている。
この記事では、L-BFGSがどのような手続きによって最適解を得ているのか簡単にまとめる。

Newton法
-------------

L-BFGSなどは準Newton法と呼ばれており、Newton法を元に作られている。
まずはNewton法について説明する。

wikipedia

最急降下法と何が違うのか？
上の図の赤い線はNewton法、緑の線は最急降下法のxの更新を表す。見てわかる通りNewton法のほうが初期値から最適解x∗まで短い距離で到達している。最急降下法では１次の勾配のみを考慮するのに対してNewton法では2次の勾配まで考慮しているため、最急降下法に比べて大域的最適解に効率よく到達することができる。

ニュートン法の仕組み
点xnの次の点をxn+1=xn+Δxとして最適な解を逐次的に求める。
xn+1は以下のように決める
勾配方向dを決定
f(x−αd)を最小にするαを決める
xn+αdをxn+1とする
最適化したい関数をf(x)とする。この関数を二次のTaylor展開で近似すると

f(x+Δx)≈f(x)+ΔxT∇f(x)+12ΔxT(∇2f(x))Δx
ここでgn=∇f(xn)は点xにおける勾配で、Hn=∇2f(xn)は点xにおけるヘッセ行列である。

上の展開式を書き直すと、

h(Δx)≈ΔxTgn+12ΔxTHnΔx
この式を最小化するΔxを求めたいので、両辺をΔxで微分すると

∂hn(Δx)∂Δx=gn+HnΔx
上の式が0になるときhn(Δx)は最小値をとるので、その時のΔxを求めると、

Δx=−H−1ngn
ここで求めたΔxを探索方向とする。

アルゴリズムをまとめると以下のようになる

アルゴリズム:Newton法

while no convergence
   H−1n,gnを計算
   d ←−H−1ngn %探索方向dを決定
   α∗=arg minαf(xn−αd) % line searchでステップ幅を決定
   xn+1←xn+α∗d

Newton法の問題点
-----------------

上の方法ではH−1nを計算する必要があるが、xの次元が∼108になった時にその行列の要素数は∼1016になってしまい、値がメモリに載らなくなってしまう。

準Newton法(L-BFGS)
--------------------

準Newton法とはNewton法におけるヘッセ行列を別の方法で置き換えた方法のこと。準Newton法は様々な種類があるが、ここではL-BFGSについて説明する。

セカント条件
--------------

ヘッセ行列の逆行列H−1nが、正定値対称行列Bnで近似できたとする。すると、

d=−Bngn
---------

は降下方向を向いていることが保証される。 ここで、gnの1次のテイラー展開を考えると

gn+1−gnxn+1−xn≈Hn
---------------------

なので、Bnも上の条件を満たすとすると、

Bn(xn−xn−1)=(gn−gn−1)Bnsn=yn
ここで、

yn=gn−gn−1sn=xn−xn−1
とする。

Bnをどのように更新するか？
Bnを更新するとき、以下の２つの条件に従うとする

BnはBn−1に対してあまり変化しない。
Bnsn=ynを満たす
つまり、

B=arg minB∥B−Bn−1∥2s.t. Bnsn=ynB:正定値対称行列
上の条件を満たすH−1n=Bの更新式はBFGS更新式と呼ばれており、以下の式で表すことができる

Bn+1=(I−ρnynsTn)Bn(I−ρnsnyTn)+ρnsnsTn
ただし

ρ=ynsTn−1

Link(参考リンク・jupyterリンク等)
---------------------------------------

3分でわかるL-BFGS
___________________

http://kotarotanahashi.github.io/blog/2015/10/03/l-bfgsfalseshi-zu-mi/
