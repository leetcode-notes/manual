scikitlearn decision tree
=============================

CART
--------

決定木のアルゴリズムとして最も頻繁に使われるのが「CART」です。

日本語で「カート」と呼ばれておりClassification and Regression Treeの略です。

各アルゴリズムではどのように決定木を分割させるかの計算方法や決定木の分岐数等が異なります。CARTでは2分岐のみですが他のアルゴリズム（例：C4.0）では3つ以上の分岐が可能

# Scikit-learnからIrisデータを読み込む


# targetでターゲットへアクセス（0〜5行のデータを表示）

.. code-block:: python

    print(iris.target[0:5])
    Out [ ]:
    [[ 5.1  3.5  1.4  0.2]
     [ 4.9  3.   1.4  0.2]
     [ 4.7  3.2  1.3  0.2]
     [ 4.6  3.1  1.5  0.2]
     [ 5.   3.6  1.4  0.2]]
    [0 0 0 0 0]

データも揃いましたので決定木のモデルを訓練してみましょう。今回は決定木の概要を理解する目的ですので2階層のシンプルな決定木を構築します。

モデルの訓練
通常の機械学習の流れでは訓練データとテストデータに切り分けて評価を行いますが今回は省力します。参考までにですが、下記のclfとはClassification（分類）の略語です。

In [ ]:
# 決定木モデルの訓練
clf = tree.DecisionTreeClassifier(max_depth=2)
clf = clf.fit(iris.data, iris.target)
これで決定木の訓練が完了です。2階層の決定木を作りますので、max_depthに「2」と指定をしています。では、訓練した決定木をgraphvizを利用して確認してみましょう。

In [ ]:
# 訓練済みの決定木の視覚化

.. code-block:: python

    dot_data = tree.export_graphviz(clf, out_file=None,
                             feature_names=iris.feature_names,
                             class_names=iris.target_names,
                             rounded = True,
                             filled=True,
                             special_characters=True)
    graph = graphviz.Source(dot_data)
    graph

1行目） データ分割の条件
最上部のノードを見ていただくと「petal width(cm)
0.8」と1行目に表記があります。こちらはこのノードにおいてのデータ分割条件です。つまりこのノードではpetal width（花弁の幅）が0.8以下かどうかを振り分けています。

（2行目） gini = ジニ不純度
ジニ不純度とは「データがどれだけ上手に分岐できるか」を表す指標となります。Giniは最大を1として値が大きいほど不純度が高い、つまり上手に分岐できていないことを意味します。

Scikit-learnの決定木ではデフォルトでジニ不純度が使われていますが、パラメーターに「entropy」と指定することで情報エントロピーという異なる種類の指標を利用することも可能です。

（3行目）samples = 観測数
最初のノードを見て頂くとsamples=150とあり、2層目の左側（橙色）のノードをみるとsamples＝50とあります。これらのsamplesとは各ノード（またはカテゴリー）に分類された観測数を表します。

アイリスのデータは全部で150ありましたので、最初のノードのSamplesは150からスタートしています。分岐条件は「花弁の幅が0.8」とあり、その分岐条件の結果、0.8以下のサンプルは50（左オレンジのノード）、0.8より大きいサンプルは100（右白のノード）とデータが分岐しているのが解ります。

（4行目） value = ノードの分類サマリー
データセットには3つのクラス（品種）があります。４行目のvalueとは各ノードの分類のサマリーを表しています。最上部のノードを確認してみると[50, 50, 50]とありますが、これはこのノード内のデータの分類が全３品種で50データずつあることを意味しています。

では実際にこの決定木を紐解いてみましょう。

1層目 ノード
    決定木の分岐条件はpetal width（花弁の長さ）が0.8cm以下かどうかです
    対象データ数は全部で150個
2層目 左ノード
    1層目の分岐条件が「True」 = 花弁の長さが0.8ｃｍ以下
    50個のデータがこの条件で振り分けられました
    その全てがIris-Setosaの品種
    このノードは結果価値を示す最終地点の結果ノード（Terminal Node）
2層目 右ノード
    1層目の分岐条件が「False」 = 花弁の長さが0.8cmより大きい
    100個のデータがこの条件で振り分けられました
    そのうち50個がIris-versicolorで50個がIris-virginicaの品種です
    このノードの分岐条件はpetal width（花弁の幅）が1.75cm以下かどうか
3層目 左ノード
    2層目の分岐条件が「True」 = 花弁の幅が1.75以下
    54個のデータがこの条件で振り分けられました
    そのうち49個がIris-versicolorで5個がIris-virginicaの品種です
    このノードは結果価値を示す最終地点の結果ノード（Terminal Node）
3層目 右ノード
    2層目の分岐条件が「False」 = 花弁の幅が1.75より大きい
    46個のデータがこの条件で振り分けられました
    そのうち45個がIris-virginicaで1個がIris-versicolorの品種です
    このノードは結果価値を示す最終地点の結果ノード（Terminal Node）
